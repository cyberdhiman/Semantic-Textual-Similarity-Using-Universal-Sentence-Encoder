# -*- coding: utf-8 -*-
"""Precily.ipynb

Automatically generated by Colaboratory.

Author:: Vishal Kumar
"""

from __future__ import absolute_import, division, print_function, unicode_literals
import tensorflow as tf #version 2.0
import tensorflow_hub as hub
from math import *
import numpy as np
import pandas as pd
#tf.random.set_seed(1)

#reading dataset from csv file 
data=pd.read_csv("Text_Similarity_Dataset.csv")

print("\n[OUT]: Top 5 rows of data:\n",data.head(5))

#url of pretrained model present in TensorFlow hub 
url="https://tfhub.dev/google/universal-sentence-encoder-large/3"

#getting model 
embed =hub.KerasLayer(url)

#getting embeddings of the text1 and text2  present in row 0
embeddings=embed(np.array([data.iloc[0]['text1'],data.iloc[0]['text2']]))

#how paragraph in row 0 looks

print("text1 of row 0: \n",data.iloc[0]['text1'])
print("text2 of row 0: \n",data.iloc[0]['text2'])

#embeddings of each text represented by 512 values
print("\n [OUT]: Emdedding(Tensor representation) of text1 in row 0: \n",embeddings[0])

print("\n Similarity of the text0 and text1 of row 0: ", np.inner(embeddings[0],embeddings[1]))
print("\n Taking four digit after decimal",round(np.inner(embeddings[0],embeddings[1]),4))

print("\n[OUT]: Total number of data Samples: ",data.shape[0])

#i have taken Similarity Score Upto 5 Places after decimal

#here I have used only trivial inner product for finiding Similarity between vectors
def calculate_STS_Inner_product():
  d={"Unique_ID":[],"Similarity_Score":[]}
  for i in range(data.shape[0]):
    embeddings=embed(np.array([data.iloc[i]['text1'],data.iloc[i]['text2']]))
    d["Unique_ID"].append(i)
    d["Similarity_Score"].append(round(max(np.inner(embeddings[0],embeddings[1]),0.0),5))
  return d

#calling the function to get Similarity Score
STS_in_p=calculate_STS_Inner_product()

print("\n[OUT]: Number of values in Similarity Score list: ",len(STS_in_p["Similarity_Score"]))

print("\n[OUT]: Semantic Textual Similarity:\n Unique_ID: ",STS_in_p["Unique_ID"])
print("\n Similarity Score: ",STS_in_p["Similarity_Score"])

#converting result STS_in_p to dataframe to convert it to csv file
dfr=pd.DataFrame(STS_in_p)
dfr.to_csv("Similarity_Score_using_inner_product.csv")

print("\n [OUT]:  Semantic Textual Similarity w.r.t to Unique ID:\n",dfr)

#since there are more than one method to calculate Similarity, We can use one of them also 
class Similarity:
  #this returns eculidean distance between two embedding vectors
  def eculidean_distance(self,V1,V2):

        return round(1/(1+sqrt(sum(pow(a-b,2) for a, b in zip(V1, V2)))),5)
  
  #this returns manhattan distance between two embedding vectors
  def manhattan_distance(self,V1,V2):
        return round(1/(1+sum(abs(a-b) for a,b in zip(V1,V2))),5)
  
  #this returns cosine similarity between two embedding vectors
  def cosine_similarity(self,V1,V2):
    numerator = sum(a*b for a,b in zip(V1,V2))
    denominator = self.square_rooted(V1)*self.square_rooted(V2)
    
    #cosine similarity is not valid if denominator=0
    if denominator==0:
      return 
    return round(numerator/float(denominator),5)

#here i am using eculidean distance to calculate distance between tensor of the text's
#lesser the distace more the are 
def calculate_STS():
  obj=Similarity()

  d={"Unique_ID":[],"Similarity_Score":[]}
  for i in range(data.shape[0]):
    embeddings=embed(np.array([data.iloc[i]['text1'],data.iloc[i]['text2']]))
    d["Unique_ID"].append(i)
    d["Similarity_Score"].append(round(obj.eculidean_distance(embeddings[0],embeddings[1]),5))
  return d

STS_Eculid=calculate_STS()

#converting result STS_Eculid to dataframe to convert it to csv file
dfr_ed=pd.DataFrame(STS_Eculid)
dfr_ed.to_csv("Similarity_Score_using_Ecuildean_Distance.csv")

print("\n [OUT]:  Semantic Textual Similarity w.r.t to Unique ID using Eculidean Distance as similarity matrix:\n",dfr_ed)
